{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "No module named skimage.io",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-fffef15eef47>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mskimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named skimage.io"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import glob\n",
    "import keras\n",
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from keras.optimizers import *\n",
    "from keras import backend as K\n",
    "from keras import metrics\n",
    "import mido\n",
    "import numpy as np\n",
    "import random\n",
    "import skimage.io\n",
    "import tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "notes = 36\n",
    "lookback = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(object):\n",
    "    def create(self, vocab_size=64, max_length=256, latent_rep_size=128, lr=0.001):\n",
    "        self.encoder = None\n",
    "        self.decoder = None\n",
    "        self.sentiment_predictor = None\n",
    "        self.autoencoder = None\n",
    "\n",
    "        x = Input(shape=(max_length, vocab_size))\n",
    "        #x_embed = Embedding(vocab_size, 64, input_length=max_length)(x)\n",
    "\n",
    "        vae_loss, encoded = self._build_encoder(x, latent_rep_size=latent_rep_size, max_length=max_length)\n",
    "        self.encoder = Model(inputs=x, outputs=encoded)\n",
    "\n",
    "        encoded_input = Input(shape=(latent_rep_size,))\n",
    "        predicted_sentiment = self._build_sentiment_predictor(encoded_input)\n",
    "        self.sentiment_predictor = Model(encoded_input, predicted_sentiment)\n",
    "\n",
    "        decoded = self._build_decoder(encoded_input, vocab_size, max_length)\n",
    "        self.decoder = Model(encoded_input, decoded)\n",
    "\n",
    "        self.autoencoder = Model(inputs=x, outputs=[self._build_decoder(encoded, vocab_size, max_length), self._build_sentiment_predictor(encoded)])\n",
    "        self.autoencoder.compile(optimizer=Adam(lr=lr),\n",
    "                                 loss=[vae_loss, 'binary_crossentropy'],\n",
    "                                 metrics=['accuracy'])\n",
    "    \n",
    "    def _build_encoder(self, x, latent_rep_size=128, max_length=None, epsilon_std=0.01):\n",
    "        h = Bidirectional(LSTM(500, return_sequences=True, name='lstm_1'), merge_mode='concat')(x)\n",
    "        h = Dropout(0.5, name='dropout_1')(h)\n",
    "        h = Bidirectional(LSTM(500, return_sequences=False, name='lstm_2'), merge_mode='concat')(h)\n",
    "        h = Dropout(0.5, name='dropout_2')(h)\n",
    "        h = Dense(435, activation='relu', name='dense_1')(h)\n",
    "\n",
    "        def sampling(args):\n",
    "            z_mean_, z_log_var_ = args\n",
    "            batch_size = K.shape(z_mean_)[0]\n",
    "            epsilon = K.random_normal(shape=(batch_size, latent_rep_size), mean=0., stddev=epsilon_std)\n",
    "            return z_mean_ + K.exp(z_log_var_ / 2) * epsilon\n",
    "\n",
    "        z_mean = Dense(latent_rep_size, name='z_mean', activation='linear')(h)\n",
    "        z_log_var = Dense(latent_rep_size, name='z_log_var', activation='linear')(h)\n",
    "\n",
    "        def vae_loss(x, x_decoded_mean):\n",
    "            x = K.flatten(x)\n",
    "            x_decoded_mean = K.flatten(x_decoded_mean)\n",
    "            xent_loss = max_length * metrics.binary_crossentropy(x, x_decoded_mean)\n",
    "            kl_loss = - 0.5 * K.mean(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
    "            return xent_loss + kl_loss\n",
    "\n",
    "        return (vae_loss, Lambda(sampling, output_shape=(latent_rep_size,), name='lambda')([z_mean, z_log_var]))\n",
    "    \n",
    "    def _build_decoder(self, encoded, vocab_size, max_length):\n",
    "        repeated_context = RepeatVector(max_length)(encoded)\n",
    "\n",
    "        h = LSTM(500, return_sequences=True, name='dec_lstm_1')(repeated_context)\n",
    "        h = LSTM(500, return_sequences=True, name='dec_lstm_2')(h)\n",
    "\n",
    "        decoded = TimeDistributed(Dense(vocab_size, activation='sigmoid'), name='decoded_mean')(h)\n",
    "\n",
    "        return decoded\n",
    "    \n",
    "    def _build_sentiment_predictor(self, encoded):\n",
    "        h = Dense(100, activation='linear')(encoded)\n",
    "\n",
    "        return Dense(64, activation='sigmoid', name='pred')(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 256\n",
    "NUM_WORDS = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=NUM_WORDS)\n",
    "\n",
    "print(\"Training data\")\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "print(\"Number of words:\")\n",
    "print(len(np.unique(np.hstack(X_train))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pad_sequences(X_train, maxlen=MAX_LENGTH)\n",
    "X_test = pad_sequences(X_test, maxlen=MAX_LENGTH)\n",
    "\n",
    "train_indices = np.random.choice(np.arange(X_train.shape[0]), 2000, replace=False)\n",
    "test_indices = np.random.choice(np.arange(X_test.shape[0]), 1000, replace=False)\n",
    "\n",
    "X_train = X_train[train_indices]\n",
    "y_train = y_train[train_indices]\n",
    "\n",
    "X_test = X_test[test_indices]\n",
    "y_test = y_test[test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = np.zeros((X_train.shape[0], MAX_LENGTH, NUM_WORDS))\n",
    "temp[np.expand_dims(np.arange(X_train.shape[0]), axis=0).reshape(X_train.shape[0], 1), np.repeat(np.array([np.arange(MAX_LENGTH)]), X_train.shape[0], axis=0), X_train] = 1\n",
    "\n",
    "X_train_one_hot = temp\n",
    "\n",
    "temp = np.zeros((X_test.shape[0], MAX_LENGTH, NUM_WORDS))\n",
    "temp[np.expand_dims(np.arange(X_test.shape[0]), axis=0).reshape(X_test.shape[0], 1), np.repeat(np.array([np.arange(MAX_LENGTH)]), X_test.shape[0], axis=0), X_test] = 1\n",
    "\n",
    "x_test_one_hot = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_checkpoint(dir, model_name):\n",
    "    filepath = dir + '/' + \\\n",
    "               model_name + \"-{epoch:02d}-{val_decoded_mean_acc:.2f}-{val_pred_loss:.2f}.h5\"\n",
    "    directory = os.path.dirname(filepath)\n",
    "\n",
    "    try:\n",
    "        os.stat(directory)\n",
    "    except:\n",
    "        os.mkdir(directory)\n",
    "\n",
    "    checkpointer = ModelCheckpoint(filepath=filepath,\n",
    "                                   verbose=1,\n",
    "                                   save_best_only=False)\n",
    "\n",
    "    return checkpointer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model = VAE()\n",
    "    model.create(vocab_size=NUM_WORDS, max_length=MAX_LENGTH)\n",
    "    model.autoencoder.summary()\n",
    "\n",
    "    checkpointer = create_model_checkpoint('models', 'rnn_ae')\n",
    "\n",
    "    model.autoencoder.fit(x=xdata, y={'decoded_mean': xdata, 'pred': ydata},\n",
    "                          batch_size=10, epochs=10, callbacks=[checkpointer],\n",
    "                          validation_data=(xtest, {'decoded_mean': xtest, 'pred':  ytest}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xdata = np.random.randint(2, size=(128, MAX_LENGTH, NUM_WORDS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ydata = np.random.randint(2, size=(128, NUM_WORDS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtest = np.random.randint(2, size=(64, MAX_LENGTH, NUM_WORDS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytest = np.random.randint(2, size=(64, NUM_WORDS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chords_from_midi(midi_file):\n",
    "    data = []\n",
    "    midi = mido.MidiFile(midi_file)\n",
    "    for track in midi.tracks:\n",
    "        if track.name == 'Chords':\n",
    "            for message in track:\n",
    "                if message.type in ['note_on', 'note_off']:\n",
    "                    data.append((1 if message.type == 'note_on' else 0, message.note, message.velocity, message.time))\n",
    "    assert data\n",
    "    return np.array(data)\n",
    "\n",
    "\n",
    "def encode_chords(sequence):\n",
    "    switches = []\n",
    "    keys = []\n",
    "    velocities = []\n",
    "    times = []\n",
    "    switch = None\n",
    "    key = None\n",
    "    velocity = None\n",
    "    time = None\n",
    "    minimum = min(sequence[:, 1])\n",
    "    for item in sequence:\n",
    "        if switch != item[0] or velocity != item[2] or item[3] != 0:\n",
    "            if switch is not None and key is not None and velocity is not None and time is not None:\n",
    "                switches.append(switch)\n",
    "                keys.append(key)\n",
    "                velocities.append([int(x) for x in format(velocity, '08b')])\n",
    "                times.append([int(x) for x in format(time, '016b')])\n",
    "            key = np.zeros((notes,), dtype=int)\n",
    "            time = item[3]\n",
    "        switch = item[0]\n",
    "        key[item[1] - minimum] = 1\n",
    "        velocity = item[2]\n",
    "    return [np.array(switches)[:, np.newaxis], np.array(keys), np.array(velocities), np.array(times)]\n",
    "\n",
    "\n",
    "def augment_chords(data):\n",
    "    augmented = []\n",
    "    assert len(set([len(data[0]), len(data[1]), len(data[2]), len(data[3])])) == 1\n",
    "    events = len(data[0])\n",
    "    \n",
    "    def high(sequence):\n",
    "        high = 0\n",
    "        for event in range(1, events):\n",
    "            high_candidate = notes - np.argmax(sequence[event, ::-1])\n",
    "            if high_candidate > high:\n",
    "                high = high_candidate\n",
    "        return high\n",
    "    \n",
    "    maximum = high(data[1])\n",
    "    transpositions = notes - maximum + 1\n",
    "    for i in range(transpositions):\n",
    "        progression = np.empty((events, notes), dtype=int)\n",
    "        for j in range(events):\n",
    "            progression[j, :] = np.concatenate((\n",
    "                np.zeros((i,)),\n",
    "                data[1][j, :maximum],\n",
    "                np.zeros((notes - maximum - i,))\n",
    "            ))\n",
    "        augmented.append([data[0], progression, data[2], data[3]])\n",
    "    return augmented\n",
    "\n",
    "\n",
    "def prepare_chords(data):\n",
    "    assert len(set([len(data[0]), len(data[1]), len(data[2]), len(data[3])])) == 1\n",
    "    sequences = len(data[0])\n",
    "    x = [np.zeros((sequences, lookback, 1), dtype=int), np.zeros((sequences, lookback, notes), dtype=int), np.zeros((sequences, lookback, 8), dtype=int), np.zeros((sequences, lookback, 16), dtype=int)]\n",
    "    y = data\n",
    "    for i in range(1, sequences):\n",
    "        x[0][i, -i:, :] = data[0][:i, :]\n",
    "        x[1][i, -i:, :] = data[1][:i, :]\n",
    "        x[2][i, -i:, :] = data[2][:i, :]\n",
    "        x[3][i, -i:, :] = data[3][:i, :]\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def load_chords(midi_dir):\n",
    "    all_data = []\n",
    "    midi_files = sorted(glob.glob(os.path.join(midi_dir, '*.mid')) + glob.glob(os.path.join(midi_dir, '*.midi')))\n",
    "    for midi_file in midi_files:\n",
    "        try:\n",
    "            data = [prepare_chords(x) for x in augment_chords(encode_chords(chords_from_midi(midi_file)))]\n",
    "            all_data.extend(data)\n",
    "        except (KeyboardInterrupt, SystemExit):\n",
    "            raise\n",
    "        except:\n",
    "            print(\"Skipping\", midi_file)\n",
    "    random.shuffle(all_data)\n",
    "    return all_data\n",
    "\n",
    "\n",
    "# def input_output(sequence):\n",
    "#     x = []\n",
    "#     y = []\n",
    "#     for i in range(len(sequence)):\n",
    "#         if i == 0:\n",
    "#             x.append(np.zeros((1, 1, 1 + notes + 8 + 16)).astype(int))\n",
    "#         elif i < lookback:\n",
    "#             x.append(np.vstack([np.zeros((1, 1 + notes + 8 + 16)), sequence[:i, :]])[np.newaxis, :, :].astype(int))\n",
    "#         else:\n",
    "#             x.append(np.vstack([np.zeros((1, 1 + notes + 8 + 16)), sequence[i - lookback:i, :]])[np.newaxis, :, :].astype(int))\n",
    "#         y.append(sequence[np.newaxis, i, :].astype(int))\n",
    "#     return (x, y)\n",
    "\n",
    "\n",
    "# def pad_sequences(data, length):\n",
    "#     x = np.zeros((1, length, 1 + notes + 8 + 16))\n",
    "#     y = np.zeros((1, 1 + notes + 8 + 16))\n",
    "#     for i, seq in enumerate(data[0]):\n",
    "#         seq_len = seq.shape[1]\n",
    "#         pad_len = length - seq_len\n",
    "#         for j in range(seq_len):\n",
    "#             x[0, length - seq_len + j, :] = seq[0, j, :]\n",
    "#         #y[0, i] = \n",
    "#     return (x, y)\n",
    "\n",
    "\n",
    "def generator(data):\n",
    "    while True:\n",
    "        for sequence in data:\n",
    "            yield (sequence[0], sequence[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = pad_sequences(input_output(encode_chords(chords_from_midi('/home/santiago/Projects/MusicGenerator/data/midi/6.mid'))), 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = chords_from_midi('/home/santiago/Projects/MusicGenerator/data/midi/6.mid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = encode_chords(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = augment_chords(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = prepare_chords(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = prepare_chords(encode_chords(chords_from_midi('/home/santiago/Projects/MusicGenerator/data/midi/6.mid')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_chords('/home/santiago/Projects/MusicGenerator/data/midi/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "midi_file = '/home/santiago/Projects/MusicGenerator/data/midi/6.mid'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [prepare_chords(x) for x in augment_chords(encode_chords(chords_from_midi(midi_file)))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = generator(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(object):\n",
    "    def create(self, data_width=1+notes+8+16, lookback=lookback, latent_rep_size=128, lr=0.001):\n",
    "        self.encoder = None\n",
    "        self.decoder = None\n",
    "        self.predictor = None\n",
    "        self.autoencoder = None\n",
    "        \n",
    "        switches_in = Input(shape=(lookback, 1), name='switches_in')\n",
    "        notes_in = Input(shape=(lookback, notes), name='notes_in')\n",
    "        velocities_in = Input(shape=(lookback, 8), name='velocities_in')\n",
    "        times_in = Input(shape=(lookback, 16), name='times_in')\n",
    "        \n",
    "        x = Concatenate(name='input_concat')([switches_in, notes_in, velocities_in, times_in])\n",
    "        \n",
    "        #x = Input(shape=(max_length, vocab_size))\n",
    "\n",
    "        vae_loss, encoded = self._build_encoder(x, latent_rep_size=latent_rep_size, lookback=lookback)\n",
    "        self.encoder = Model(inputs=[switches_in, notes_in, velocities_in, times_in], outputs=encoded)\n",
    "\n",
    "        encoded_input = Input(shape=(latent_rep_size,))\n",
    "        predicted = self._build_predictor(encoded_input)\n",
    "        self.predictor = Model(encoded_input, predicted)\n",
    "\n",
    "        decoded = self._build_decoder(encoded_input, data_width, lookback)\n",
    "        self.decoder = Model(encoded_input, decoded)\n",
    "        \n",
    "        autoencoder_outputs = [self._build_decoder(encoded, data_width, lookback)]\n",
    "        autoencoder_outputs.extend(self._build_predictor(encoded))\n",
    "        \n",
    "        self.autoencoder = Model(inputs=[switches_in, notes_in, velocities_in, times_in], outputs=autoencoder_outputs)\n",
    "        self.autoencoder.compile(optimizer=Adam(lr=lr),\n",
    "                                 loss=[vae_loss, 'binary_crossentropy', 'binary_crossentropy', 'binary_crossentropy', 'binary_crossentropy'],\n",
    "                                 metrics=['accuracy'])\n",
    "    \n",
    "    def _build_encoder(self, x, latent_rep_size=128, lookback=None, epsilon_std=0.01):\n",
    "        h = Bidirectional(LSTM(500, return_sequences=True), merge_mode='concat', name='bidirectional_1')(x)\n",
    "        h = Dropout(0.5, name='dropout_1')(h)\n",
    "        h = Bidirectional(LSTM(500, return_sequences=False), merge_mode='concat', name='bidirectional_2')(h)\n",
    "        h = Dropout(0.5, name='dropout_2')(h)\n",
    "        h = Dense(435, activation='relu', name='dense_1')(h)\n",
    "\n",
    "        def sampling(args):\n",
    "            z_mean_, z_log_var_ = args\n",
    "            batch_size = K.shape(z_mean_)[0]\n",
    "            epsilon = K.random_normal(shape=(batch_size, latent_rep_size), mean=0., stddev=epsilon_std)\n",
    "            return z_mean_ + K.exp(z_log_var_ / 2) * epsilon\n",
    "\n",
    "        z_mean = Dense(latent_rep_size, name='z_mean', activation='linear')(h)\n",
    "        z_log_var = Dense(latent_rep_size, name='z_log_var', activation='linear')(h)\n",
    "\n",
    "        def vae_loss(x, x_decoded_mean):\n",
    "            x = K.flatten(x)\n",
    "            x_decoded_mean = K.flatten(x_decoded_mean)\n",
    "            xent_loss = lookback * metrics.binary_crossentropy(x, x_decoded_mean)\n",
    "            kl_loss = - 0.5 * K.mean(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
    "            return xent_loss + kl_loss\n",
    "\n",
    "        return (vae_loss, Lambda(sampling, output_shape=(latent_rep_size,), name='lambda')([z_mean, z_log_var]))\n",
    "    \n",
    "    def _build_decoder(self, encoded, data_width, lookback):\n",
    "        repeated_context = RepeatVector(lookback, name='repeat_vector')(encoded)\n",
    "\n",
    "        h = LSTM(500, return_sequences=True, name='dec_lstm_1')(repeated_context)\n",
    "        h = LSTM(500, return_sequences=True, name='dec_lstm_2')(h)\n",
    "\n",
    "        decoded = TimeDistributed(Dense(data_width, activation='sigmoid'), name='decoded_mean')(h)\n",
    "\n",
    "        return decoded\n",
    "    \n",
    "    def _build_predictor(self, encoded):\n",
    "        h = Dense(100, activation='linear', name='dense_2')(encoded)\n",
    "        \n",
    "        switches_out = Dense(1, activation='sigmoid', name='switches_out')(h)\n",
    "        notes_out = Dense(notes, activation='sigmoid', name='notes_out')(h)\n",
    "        velocities_out = Dense(8, activation='sigmoid', name='velocities_out')(h)\n",
    "        times_out = Dense(16, activation='sigmoid', name='times_out')(h)\n",
    "        \n",
    "        return switches_out, notes_out, velocities_out, times_out\n",
    "        \n",
    "        #return Dense(data_width, activation='sigmoid', name='pred')(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VAE()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_list = []\n",
    "output_list.extend(data[0][1])\n",
    "#test = np.concatenate(data[0][0], axis=2)\n",
    "output_list.append(np.concatenate(data[0][1], axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0][1][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.autoencoder.fit(data[0][0], output_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
